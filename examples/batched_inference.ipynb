{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb5654-fafa-463b-9080-de1d83ec3234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8ac7df-1be9-4157-937f-40e812e37743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from StaICC.prefabricate_inference import model_kernel\n",
    "import functools\n",
    "from StaICC.prefabricate_inference import standard_calibration\n",
    "\n",
    "oriinference = functools.partial(model_kernel.standard_ICL_inference_with_torch_Causal_LM, model = model, tokenizer = tokenizer, cache_empty = torch.cuda.empty_cache) \n",
    "batched_calibraiton = standard_calibration.batch_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192a0963-2c8f-4b2b-b114-3c4a78e6da20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "1 in 10 Data loaded:  GLUE-SST2 \n",
      "\n",
      "2 in 10 Data loaded:  rotten_tomatoes \n",
      "\n",
      "3 in 10 Data loaded:  financial_phrasebank \n",
      "\n",
      "4 in 10 Data loaded:  SST5 \n",
      "\n",
      "5 in 10 Data loaded:  TREC \n",
      "\n",
      "6 in 10 Data loaded:  AGNews \n",
      "\n",
      "7 in 10 Data loaded:  Subjective \n",
      "\n",
      "8 in 10 Data loaded:  tweet_eval_emotion \n",
      "\n",
      "9 in 10 Data loaded:  tweet_eval_hate \n",
      "\n",
      "10 in 10 Data loaded:  hate_speech_18 \n",
      "\n",
      "Data loaded successfully.\n",
      "\n",
      "Initializing experimentor on k = 4...\n",
      "\n",
      "Ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from StaICC import Normal\n",
    "\n",
    "benchmark = Normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f52d5b-cca1-44cc-b1c2-e24d22d95a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference = functools.partial(model_kernel.batched_ICL_inference_with_torch_Causal_LM, model = model, tokenizer = tokenizer, cache_empty = torch.cuda.empty_cache, batch_calibration_function = batched_calibraiton, inside_calibration_function = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3dd452d-d2da-4b96-aea2-bf70eb942344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Experiment 1 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: GLUE-SST2-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 2 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: rotten_tomatoes-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 3 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: financial_phrasebank-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 4 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: SST5-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 5 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: TREC-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 6 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: AGNews-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 7 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: Subjective-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 8 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: tweet_eval_emotion-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 9 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: tweet_eval_hate-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 10 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function batched_ICL_inference_with_torch_Causal_LM at 0x1528fa6c54e0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x15290fbcf7e0>, batch_calibration_function=<function batch_calibration at 0x1528fa6c6340>, inside_calibration_function=None) on the dataset: hate_speech_18-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
     ]
    }
   ],
   "source": [
    "output_dict = benchmark.auto_run(inference, batched_inference = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa00189-5491-449c-887a-c6728d370855",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'GLUE-SST2': {'accuracy': 0.892578125,\n",
       "   'averaged_truelabel_likelihood': 0.6026782470056837,\n",
       "   'macro_F1': 0.8900257752089354,\n",
       "   'expected_calibration_error_1': 0.27742458025724326},\n",
       "  'rotten_tomatoes': {'accuracy': 0.8935546875,\n",
       "   'averaged_truelabel_likelihood': 0.6260070392184819,\n",
       "   'macro_F1': 0.893289992590645,\n",
       "   'expected_calibration_error_1': 0.2537688381725267},\n",
       "  'financial_phrasebank': {'accuracy': 0.6279296875,\n",
       "   'averaged_truelabel_likelihood': 0.36374198126304413,\n",
       "   'macro_F1': 0.6294587699511298,\n",
       "   'expected_calibration_error_1': 0.24434038256477103},\n",
       "  'SST5': {'accuracy': 0.392578125,\n",
       "   'averaged_truelabel_likelihood': 0.2223879498714185,\n",
       "   'macro_F1': 0.32325495789286707,\n",
       "   'expected_calibration_error_1': 0.12589697617477372},\n",
       "  'TREC': {'accuracy': 0.5234375,\n",
       "   'averaged_truelabel_likelihood': 0.2155780701817084,\n",
       "   'macro_F1': 0.49280136556240084,\n",
       "   'expected_calibration_error_1': 0.2637894145023877},\n",
       "  'AGNews': {'accuracy': 0.7587890625,\n",
       "   'averaged_truelabel_likelihood': 0.3674371816156608,\n",
       "   'macro_F1': 0.7613685160453185,\n",
       "   'expected_calibration_error_1': 0.3611114361777944},\n",
       "  'Subjective': {'accuracy': 0.59765625,\n",
       "   'averaged_truelabel_likelihood': 0.5151544935936344,\n",
       "   'macro_F1': 0.5976009889053367,\n",
       "   'expected_calibration_error_1': 0.044824283615789176},\n",
       "  'tweet_eval_emotion': {'accuracy': 0.595703125,\n",
       "   'averaged_truelabel_likelihood': 0.2994968598567042,\n",
       "   'macro_F1': 0.484591006852022,\n",
       "   'expected_calibration_error_1': 0.2615837134245932},\n",
       "  'tweet_eval_hate': {'accuracy': 0.533203125,\n",
       "   'averaged_truelabel_likelihood': 0.4967948269418624,\n",
       "   'macro_F1': 0.4227053140096618,\n",
       "   'expected_calibration_error_1': 0.04702624916890291},\n",
       "  'hate_speech_18': {'accuracy': 0.7734375,\n",
       "   'averaged_truelabel_likelihood': 0.25006540127849974,\n",
       "   'macro_F1': 0.23773200303217523,\n",
       "   'expected_calibration_error_1': 0.5395743072841461}},\n",
       " {'accuracy': 0.65888671875,\n",
       "  'averaged_truelabel_likelihood': 0.39593420508266985,\n",
       "  'macro_F1': 0.5732828690050493,\n",
       "  'expected_calibration_error_1': 0.24193401813429283})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
