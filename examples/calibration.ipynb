{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0cb5654-fafa-463b-9080-de1d83ec3234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2320415/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8ac7df-1be9-4157-937f-40e812e37743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from StaICC.prefabricate_inference import model_kernel\n",
    "import functools\n",
    "from StaICC.prefabricate_inference import standard_calibration\n",
    "\n",
    "oriinference = functools.partial(model_kernel.standard_ICL_inference_with_torch_Causal_LM, model = model, tokenizer = tokenizer, cache_empty = torch.cuda.empty_cache, return_hidden_state = True, return_full_vocab_prob = False) \n",
    "calibration = standard_calibration.hidden_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192a0963-2c8f-4b2b-b114-3c4a78e6da20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "1 in 10 Data loaded:  GLUE-SST2 \n",
      "\n",
      "2 in 10 Data loaded:  rotten_tomatoes \n",
      "\n",
      "3 in 10 Data loaded:  financial_phrasebank \n",
      "\n",
      "4 in 10 Data loaded:  SST5 \n",
      "\n",
      "5 in 10 Data loaded:  TREC \n",
      "\n",
      "6 in 10 Data loaded:  AGNews \n",
      "\n",
      "7 in 10 Data loaded:  Subjective \n",
      "\n",
      "8 in 10 Data loaded:  tweet_eval_emotion \n",
      "\n",
      "9 in 10 Data loaded:  tweet_eval_hate \n",
      "\n",
      "10 in 10 Data loaded:  hate_speech_18 \n",
      "\n",
      "Data loaded successfully.\n",
      "\n",
      "Initializing experimentor on k = 4...\n",
      "\n",
      "Ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from StaICC import Normal\n",
    "\n",
    "benchmark = Normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f52d5b-cca1-44cc-b1c2-e24d22d95a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "1\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "2\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "3\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "4\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "5\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "6\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "7\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "8\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Calibration Training Finished.\n",
      "\n",
      "9\n",
      "Process: 100%, 128 in 128 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2320415/StaICC_Expe/StaICC/prefabricate_inference/standard_calibration.py:172: UserWarning: Some categories didn't present in the calibration set: hate_speech_18-calibration\n",
      "  warnings.warn(\"Some categories didn't present in the calibration set: \" + str(calibration_set.get_dataset_name()))\n"
     ]
    }
   ],
   "source": [
    "cali = []\n",
    "for i in range(len(benchmark.experimentor)):\n",
    "    print(i)\n",
    "    tempcali = calibration(benchmark.experimentor[i].get_label_space())\n",
    "    tempcali.train(benchmark.experimentor[i].get_prompt_writter_from_dataline(), oriinference, benchmark.experimentor[i].calibration_set(), calibration_number = 128)\n",
    "    cali.append(tempcali)\n",
    "    \n",
    "inference = []\n",
    "for i in range(len(benchmark.experimentor)):\n",
    "    inference.append(functools.partial(model_kernel.standard_ICL_inference_with_torch_Causal_LM, model = model, tokenizer = tokenizer, cache_empty = torch.cuda.empty_cache, calibration_function = cali[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3dd452d-d2da-4b96-aea2-bf70eb942344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Experiment 1 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14abba4b3250>) on the dataset: GLUE-SST2-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 2 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14aba4951350>) on the dataset: rotten_tomatoes-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 3 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14abbb87d310>) on the dataset: financial_phrasebank-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 4 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14aba47f6f90>) on the dataset: SST5-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 5 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14aba46730d0>) on the dataset: TREC-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 6 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14aba4686690>) on the dataset: AGNews-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 7 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14aba46392d0>) on the dataset: Subjective-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 8 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14aba4607f90>) on the dataset: tweet_eval_emotion-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 9 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14aba47f7a10>) on the dataset: tweet_eval_hate-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Experiment 10 in 10\n",
      "\n",
      "Start testing the forward inference function functools.partial(<function standard_ICL_inference_with_torch_Causal_LM at 0x14ad5c9c87c0>, model=GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "), tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, cache_empty=<function empty_cache at 0x14abbf1a9620>, calibration_function=<StaICC.prefabricate_inference.standard_calibration.hidden_calibration object at 0x14abbaa5fc50>) on the dataset: hate_speech_18-test with bias type: None.\n",
      "\n",
      "Process: 100%, 1024 in 1024 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
     ]
    }
   ],
   "source": [
    "output_dict = benchmark(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2ef20c-cd33-4a83-9be7-baa56f631905",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Divided results': {'GLUE-SST2': {'accuracy': 0.5048828125,\n",
       "   'averaged_truelabel_likelihood': 0.5013245637613665,\n",
       "   'macro_F1': 0.5046329029137052,\n",
       "   'expected_calibration_error_1': 0.3197234956710516},\n",
       "  'rotten_tomatoes': {'accuracy': 0.5244140625,\n",
       "   'averaged_truelabel_likelihood': 0.5176262426345551,\n",
       "   'macro_F1': 0.52365041384284,\n",
       "   'expected_calibration_error_1': 0.39269467683968473},\n",
       "  'financial_phrasebank': {'accuracy': 0.47265625,\n",
       "   'averaged_truelabel_likelihood': 0.47562066489317695,\n",
       "   'macro_F1': 0.35554833377312245,\n",
       "   'expected_calibration_error_1': 0.3500322328546429},\n",
       "  'SST5': {'accuracy': 0.20703125,\n",
       "   'averaged_truelabel_likelihood': 0.1949255241884676,\n",
       "   'macro_F1': 0.18123741430123225,\n",
       "   'expected_calibration_error_1': 0.2560959902764165},\n",
       "  'TREC': {'accuracy': 0.2900390625,\n",
       "   'averaged_truelabel_likelihood': 0.2621657558748818,\n",
       "   'macro_F1': 0.2465559448620819,\n",
       "   'expected_calibration_error_1': 0.41508126881429863},\n",
       "  'AGNews': {'accuracy': 0.3447265625,\n",
       "   'averaged_truelabel_likelihood': 0.31685713667603305,\n",
       "   'macro_F1': 0.3250181869272355,\n",
       "   'expected_calibration_error_1': 0.31167554621174226},\n",
       "  'Subjective': {'accuracy': 0.556640625,\n",
       "   'averaged_truelabel_likelihood': 0.5415159626508369,\n",
       "   'macro_F1': 0.5560292303167127,\n",
       "   'expected_calibration_error_1': 0.27329431254193176},\n",
       "  'tweet_eval_emotion': {'accuracy': 0.294921875,\n",
       "   'averaged_truelabel_likelihood': 0.26874091681211043,\n",
       "   'macro_F1': 0.2168448861781265,\n",
       "   'expected_calibration_error_1': 0.4650224864478158},\n",
       "  'tweet_eval_hate': {'accuracy': 0.544921875,\n",
       "   'averaged_truelabel_likelihood': 0.5438403375189669,\n",
       "   'macro_F1': 0.4892779012171262,\n",
       "   'expected_calibration_error_1': 0.43768036069673366},\n",
       "  'hate_speech_18': {'accuracy': 0.1015625,\n",
       "   'averaged_truelabel_likelihood': 0.1589679742147366,\n",
       "   'macro_F1': 0.046477418239808936,\n",
       "   'expected_calibration_error_1': 0.8184109853751996}},\n",
       " 'Averaged results': {'accuracy': 0.3841796875,\n",
       "  'averaged_truelabel_likelihood': 0.37815850792251315,\n",
       "  'macro_F1': 0.3445272632571992,\n",
       "  'expected_calibration_error_1': 0.4039711355729517}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
